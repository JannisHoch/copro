{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-model initialisation and execution\n",
    "\n",
    "In this notebook, we will show how CoPro creates, trains, and tests a machine-learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "Start with loading the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copro import utils, pipeline, evaluation, plots, machine_learning\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sbs\n",
    "import os, sys\n",
    "from sklearn import metrics\n",
    "from shutil import copyfile\n",
    "import warnings\n",
    "import glob\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better reproducibility, the version numbers of all key packages are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)]\n",
      "copro version: 0.0.8b\n",
      "geopandas version: 0.8.0\n",
      "xarray version: 0.15.1\n",
      "rasterio version: 1.1.0\n",
      "pandas version: 1.0.3\n",
      "numpy version: 1.18.1\n",
      "scikit-learn version: 0.23.2\n",
      "matplotlib version: 3.2.1\n",
      "seaborn version: 0.11.0\n",
      "rasterstats version: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "utils.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to also run this notebooks, some of the previously saved data needs to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_gdf = gpd.read_file(os.path.join('temp_files', 'conflicts.shp'))\n",
    "selected_polygons_gdf = gpd.read_file(os.path.join('temp_files', 'polygons.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_arr = np.load(os.path.join('temp_files', 'global_df.npy'), allow_pickle=True)\n",
    "global_df = pd.DataFrame(data=global_arr, columns=['geometry', 'ID'])\n",
    "global_df.set_index(global_df.ID, inplace=True)\n",
    "global_df.drop(['ID'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The configurations-file (cfg-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to continue the simulation with the same settings as in the previous notebook, the cfg-file has to be read again and the model needs to be initialised subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_file = 'example_settings.cfg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### CoPro version 0.0.8b ####\n",
      "#### For information about the model, please visit https://copro.readthedocs.io/ ####\n",
      "#### Copyright (2020-2021): Jannis M. Hoch, Sophie de Bruin, Niko Wanders ####\n",
      "#### Contact via: j.m.hoch@uu.nl ####\n",
      "#### The model can be used and shared under the MIT license ####\n",
      "\n",
      "INFO: verbose mode on: True\n",
      "INFO: saving output to folder C:\\Users\\hoch0001\\Documents\\_code\\copro\\example\\./OUT\n",
      "DEBUG: remove files in C:\\Users\\hoch0001\\Documents\\_code\\copro\\example\\OUT\n",
      "DEBUG: sparing XY.npy\n",
      "DEBUG: remove files in C:\\Users\\hoch0001\\Documents\\_code\\copro\\example\\OUT\\clfs\n",
      "DEBUG: remove files in C:\\Users\\hoch0001\\Documents\\_code\\copro\\example\\OUT\\files\n"
     ]
    }
   ],
   "source": [
    "config, out_dir, root_dir = utils.initiate_setup(settings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, conflict at the last year of the simulation period was stored temporarily to another folder than the output folder. Now let's copy these files back to the folder where the belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conflicts_in_2015.cpg\n",
      "conflicts_in_2015.dbf\n",
      "conflicts_in_2015.shp\n",
      "conflicts_in_2015.shx\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk('temp_files'):\n",
    "    files = glob.glob(os.path.abspath('./temp_files/conflicts_in*'))\n",
    "    for file in files:\n",
    "        fname = file.rsplit('\\\\')[-1]\n",
    "        print(fname)\n",
    "        copyfile(os.path.join('temp_files', fname),\n",
    "                 os.path.join(out_dir, 'files', str(fname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the XY-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no XY-data is specified in the config-file initially, we have to set this manually. Note that this de-tour is only necessary due to the splitting of the workflow in different notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set('pre_calc', 'XY', str(os.path.join(os.path.abspath(config.get('general', 'output_dir')), 'XY.npy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double-check, see if this file actually exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(config.get('pre_calc', 'XY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scence is set now and we can create the X-array and Y-array from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = pipeline.create_XY(config, out_dir, root_dir, selected_polygons_gdf, conflict_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler and classifier\n",
    "\n",
    "### Background\n",
    "\n",
    "In principle, one can put all kind of data into the samples matrix X, leading to a wide spread of magnitudes, units, statistics etc. It is therefore needed to scale (or transform) the data in the X-array such that sensible comparisons and computations are possible. To that end, a scaling technique is applied.\n",
    "\n",
    "Once there is a scaled X-array, a machine-learning model can be fitted with it together with the target values Y. \n",
    "\n",
    "### Implementation\n",
    "\n",
    "CoPro supports four different scaling techniques. For more info, see the [scikit-learn documentation](https://scikit-learn.org/stable/getting_started.html#transformers-and-pre-processors).\n",
    "\n",
    "1. MinMaxScaler;\n",
    "2. StandardScaler;\n",
    "3. RobustScaler;\n",
    "4. QuantileTransformer.\n",
    "\n",
    "From the wide range of machine-learning model, CoPro employs three different ones from the categorie of [supervised learning](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "\n",
    "1. NuSVC;\n",
    "1. KNeighborsClassifier;\n",
    "1. RFClassifier.\n",
    "\n",
    "Note that CoPro uses pretty much the default parameterization of the scalers and models. An extensive [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) did not show any significant improvements when changing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, clf = pipeline.prepare_ML(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As scaling technique, it is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as classifier, it was chosen for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output initialization\n",
    "\n",
    "Since the model is run multiple times, we need to initialize a few lists first to append the output per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_X_df = evaluation.init_out_df()\n",
    "out_y_df = evaluation.init_out_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = evaluation.init_out_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trps, aucs, mean_fpr = evaluation.init_out_ROC_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-model execution\n",
    "\n",
    "The pudels kern! This is where the magic happens, and not only once. To make sure that any conincidental results are ruled out, we run the model multiple times. Thereby, always different parts of the XY-array are used for training and prediction. By using a sufficient number of runs and averaging the overall results, we should be able to get a good picture of what the model is capable of.\n",
    "\n",
    "Per repetition, the model is evaluated. The main evaluation metric is the mean ROC-score and [**ROC-curve**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html), plotted at the end of all runs. Additional evaluation metrics are computed as described below.\n",
    "\n",
    "Also, CoPro keeps track of polygon-IDs and geometry througout this process. This is possible by keeping them associated to the actual samples matrix (X) for the entire simulation process besides the two times where they need to be separated: the training and test moment. As this is a very technical process, we here refer to the [API](https://copro.readthedocs.io/en/latest/api/XYdata.html) description of CoPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #- create plot instance\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
    "\n",
    "#- go through all n model executions\n",
    "for n in range(config.getint('settings', 'n_runs')):\n",
    "    \n",
    "    print('INFO: run {} of {}'.format(n+1, config.getint('settings', 'n_runs')))\n",
    "\n",
    "    #- run machine learning model and return outputs\n",
    "    X_df, y_df, eval_dict = pipeline.run_reference(X, Y, config, scaler, clf, out_dir, run_nr=n+1)\n",
    "    \n",
    "    #- select sub-dataset with only datapoints with observed conflicts\n",
    "    X1_df, y1_df = utils.get_conflict_datapoints_only(X_df, y_df)\n",
    "    \n",
    "    #- append per model execution\n",
    "    out_X_df = evaluation.fill_out_df(out_X_df, X_df)\n",
    "    out_y_df = evaluation.fill_out_df(out_y_df, y_df)\n",
    "    out_dict = evaluation.fill_out_dict(out_dict, eval_dict)\n",
    "\n",
    "    #- plot ROC curve per model execution\n",
    "    tprs, aucs = plots.plot_ROC_curve_n_times(ax1, clf, X_df.to_numpy(), y_df.y_test.to_list(),\n",
    "                                                                  trps, aucs, mean_fpr)\n",
    "\n",
    "#- plot mean ROC curve\n",
    "plots.plot_ROC_curve_n_mean(ax1, tprs, aucs, mean_fpr)\n",
    "\n",
    "plt.savefig('../docs/_static/roc_curve.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "### For all data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the model runs, the computed model evaluation scores per model execution were stored to a dictionary. Currently, the evaluation scores used are:\n",
    "\n",
    "* [**Accuracy**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html): the fraction of correct predictions;\n",
    "* [**Precision**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html): the ratio *tp / (tp + fp)* where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative;\n",
    "* [**Recall**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html): the ratio *tp / (tp + fn)* where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples;\n",
    "* [**F1 score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html): the F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0;\n",
    "* [**Cohen-Kappa score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html): is used to measure inter-rater reliability. It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance.\n",
    "* [**Brier score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html): the smaller the Brier score, the better, hence the naming with “loss”. The lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier loss score is relatively sensitive for imbalanced datasets;\n",
    "* [**ROC score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html): a value of 0.5 suggests no skill, e.g. a curve along the diagonal, whereas a value of 1.0 suggests perfect skill, all points along the left y-axis and top x-axis toward the top left corner. A value of 0.0 suggests perfectly incorrect predictions. Note that the ROC score is relatively insensitive for imbalanced datasets.\n",
    "\n",
    "Let's check the mean scores over all runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in out_dict:\n",
    "    \n",
    "    print('average {0} of run with {1} repetitions is {2:0.3f}'.format(key, config.getint('settings', 'n_runs'), np.mean(out_dict[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.metrics_distribution(out_dict, figsize=(20, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on all data points, the [**confusion matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) can be plotted. This is a relatively straightforward way to visualize how good the classifier values are predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "metrics.plot_confusion_matrix(clf, out_X_df.to_numpy(), out_y_df.y_test.to_list(), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per unique polygon\n",
    "\n",
    "Thus far, we merely looked at numerical scores. This of course tells us a lot about the quality of the machine-learning model and its predictions, but not so much about how this looks like spatially. We therefore combine the observations and predictions made with the associated polygons based on a 'global' dataframe functioning as a look-up table. By this means, each model output can be connected to its polygon using a unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hit, gdf_hit = evaluation.polygon_model_accuracy(out_y_df, global_df, out_dir=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a look at how often each polygon occurs in the all test samples, i.e. those obtained by appending the test samples per model execution. Besides, the overall relative distribution is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "gdf_hit.plot(ax=ax1, column='nr_predictions', legend=True, cmap='Blues')\n",
    "selected_polygons_gdf.boundary.plot(ax=ax1, color='0.5')\n",
    "ax1.set_title('number of predictions made per polygon')\n",
    "sbs.distplot(df_hit.nr_predictions.values, ax=ax2)\n",
    "ax2.set_title('distribution of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By repeating the model n times, the aim is to represent all polygons in the resulting test sample. The fraction is computed below. \n",
    "\n",
    "Note that is should be close to 100 % but may be slightly less. This can happen if input variables have no data for one polygon, leading to a removal of those polygons from the analysis. Or because some polygons and input data may not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:0.2f} % of all active polygons are considered in test sample'.format(len(gdf_hit)/len(selected_polygons_gdf)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By aggregating results per polygon, we can now assess model output spatially. Three main aspects are presented here:\n",
    "\n",
    "1. The chance of a correct prediction, defined as the ratio of number of correct predictions made to overall number of predictions made;\n",
    "2. The total number of conflicts in the test data;\n",
    "3. The chance of conflict, defined as the ration of number of conflict predictions to overall number of predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "gdf_hit.plot(ax=ax1, column='fraction_correct_predictions', legend=True, \n",
    "             legend_kwds={'label': \"fraction_correct_predictions\", 'orientation': \"horizontal\"})\n",
    "selected_polygons_gdf.boundary.plot(ax=ax1, color='0.5')\n",
    "gdf_hit.plot(ax=ax2, column='nr_observed_conflicts', legend=True, cmap='Reds', \n",
    "             legend_kwds={'label': \"nr_observed_conflicts\", 'orientation': \"horizontal\"})\n",
    "selected_polygons_gdf.boundary.plot(ax=ax2, color='0.5')\n",
    "gdf_hit.plot(ax=ax3, column='chance_of_conflict', legend=True, cmap='Blues', vmin=0, vmax=1, \n",
    "             legend_kwds={'label': \"chance of conflict\", 'orientation': \"horizontal\"})\n",
    "selected_polygons_gdf.boundary.plot(ax=ax3, color='0.5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have trained and tested our model with various combinations of data. Subsequently, the average performance of the model was evaluated with a range of metrics.\n",
    "\n",
    "If we want to re-use our model for the future and want to make projections, it is necessary to save the model (that is, the fitted classifier). It can then be loaded and one or more projections can be made with other variable values than those used for this reference run.\n",
    "\n",
    "To that end, the classifier is fitted again, but then with all data, i.e. without a split-sample test. That way, the classifier fit is most robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = machine_learning.pickle_clf(scaler, clf, config, root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this fitted classifier, we can also determine the relative feature importance of each feature. This helps getting an idea which factor is the strongest predictor of conflict risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.factor_importance(clf, config, out_dir, figsize=(10, 5));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
