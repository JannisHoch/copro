{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unravelling the complex interplay between environmental drivers and conflict\n",
    "\n",
    "This notebook contains a already relatively mature pipeline to assess the accuracy of different scale- and modelalgorithms.\n",
    "\n",
    "**Note:** This notbook is under constant development. Please be aware of the version number of the conflict model used in each of the notebooks.\n",
    "\n",
    "In its current form, we first make a selection of conflicts to be used for training and testing the model. Selection criteria are amongst others minimum number of fatalities and climate zones. Subsequently, annual statistics (now: mean) of a range of environmental variables are determined per geographic unit (now: water provinces) and stored along with a 0/1 conflict value. The number of variables to be sampled is flexible and can be easily adapted in the cfg-file. \n",
    "\n",
    "This dataset is subsequently scaled, split, and applied in a machine learning model. Since there are various ways out there to scale your data and then fit and predict, the notebook is designed such that a variable amount of scalers and models can be specified. All possible combinations of scaler and model are then evaluated and output saved to an output directory.\n",
    "\n",
    "All model settings need to be defined in the run_settings.cfg file.\n",
    "\n",
    "For questions, please contact J.M. Hoch (j.m.hoch@uu.nl).\n",
    "\n",
    "With contributions from N. Wanders (Utrecht University) and S. de Bruin (PBL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and file with settings\n",
    "\n",
    "Import all required python packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conflict_model\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "import netCDF4 as nc\n",
    "import rasterstats as rstats\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import seaborn as sbs\n",
    "from sklearn import svm, neighbors, naive_bayes, preprocessing, model_selection, metrics, utils\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better reproducibility, the version numbers of all key packages are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_model.utils.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geopandas versions lower than 0.7.0 do not yet have the clip function. The notebook will thus not work with these versions. If the environment.yml file is used to create a separate conda-environment, this should not raise a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpd.__version__ < '0.7.0':\n",
    "    sys.exit('please upgrade geopandas to version 0.7.0, your current version is {}'.format(gpd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file all the settings for the analysis are defined. By 'parsing' it, all values are read for different sections. This is a simple way to make the code independent of the input data and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_file = r'../data/run_setting.cfg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this settings-file, the set-up of the run can be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, out_dir = conflict_model.utils.initiate_setup(settings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter conflicts\n",
    "\n",
    "Not all conflicts of the database should be used for the model. This can be, for example, because they belong to a non-relevant type of conflict we are not interested in, or because it is very likely that the conflict is not water-related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the conflict data base and convert it into a georeferenced dataframe. This is needed for all following steps where this data is combined with other data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = conflict_model.utils.get_geodataframe(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, get the subset of conflicts based on user-defined conditions in the settings file. To filter out non-water-related conflicts, we use only those conflicts falling in climate zones notoriously known for water shortages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_gdf, extent_gdf = conflict_model.selection.select(gdf, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a brief glimpse at the spatial distribution of the selected conflicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = conflict_gdf.plot(figsize=(20, 10), c='r', column='best', cmap='magma', vmin=int(config.get('conflict', 'min_nr_casualties')), vmax=conflict_gdf.best.mean(), legend=True, legend_kwds={'label': \"# casualties\",})\n",
    "ax = extent_gdf.boundary.plot(ax=ax)\n",
    "ax.set_title('conflict distribution; # conflicts {}; threshold casualties {}; type of violence {}'.format(len(conflict_gdf), config.get('conflict', 'min_nr_casualties'), config.get('conflict', 'type_of_violence')))\n",
    "plt.savefig(os.path.join(out_dir, 'conflict_and_casualties_distribution.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis per year\n",
    "\n",
    "This is an essential part of the code. Here, we go through all model years as specified in the settings-file and do the following:\n",
    "\n",
    "1. Get a 0/1 classifier whether a conflict took place in a geographical unit or not;\n",
    "2. Loop through various files with environmental variables and get mean variable value per geographical unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all stored in a dictionary for easy processing. We first need to initialize this dictionary containing a pandas Series per provided environmental variable. To keep this automated, each 'key' in the section 'env_var' in the cfg-file should be equal to the variable name used on the corresponding nc-file. Then, add a pandas Series for the conflict data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = {}\n",
    "for key in config.items('env_vars'):\n",
    "    XY[str(key[0])] = pd.Series(dtype=float)\n",
    "XY['conflict'] = pd.Series(dtype=int)\n",
    "XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go through all years and all files and data and assign the values to the corresponding Series in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "print('simulation period from', str(config.getint('settings', 'y_start')), 'to', str(config.getint('settings', 'y_end')))\n",
    "print('')\n",
    "\n",
    "# go through all simulation years as specified in config-file\n",
    "for sim_year in np.arange(config.getint('settings', 'y_start'), config.getint('settings', 'y_end'), 1):\n",
    "    \n",
    "    print('entering year {}'.format(sim_year) + os.linesep)\n",
    "    \n",
    "    # go through all keys in dictionary\n",
    "    for key, value in XY.items():\n",
    "        \n",
    "        if key == 'conflict':\n",
    "            data_series = value\n",
    "            data_list = conflict_model.get_boolean_conflict.conflict_in_year_bool(conflict_gdf, extent_gdf, config, sim_year)\n",
    "            data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "            XY[key] = data_series\n",
    "            \n",
    "        else:\n",
    "            nc_fo = os.path.join(config.get('general', 'input_dir'), \n",
    "                                 config.get('env_vars', key))\n",
    "            \n",
    "            print('calculating mean {0} per aggregation unit from file {1} for year {2}'.format(key, nc_fo, sim_year))\n",
    "\n",
    "            nc_ds = xr.open_dataset(nc_fo)\n",
    "            \n",
    "            if (np.dtype(nc_ds.time) == np.float32) or (np.dtype(nc_ds.time) == np.float64):\n",
    "                data_series = value\n",
    "                data_list = conflict_model.get_var_from_nc.nc_with_float_timestamp(extent_gdf, config, key, sim_year)\n",
    "                data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "                XY[key] = data_series\n",
    "                \n",
    "            elif np.dtype(nc_ds.time) == 'datetime64[ns]':\n",
    "                data_series = value\n",
    "                data_list = conflict_model.get_var_from_nc.nc_with_continous_datetime_timestamp(extent_gdf, config, key, sim_year)\n",
    "                data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "                XY[key] = data_series\n",
    "                \n",
    "            else:\n",
    "                raise Warning('this nc-file does have a different dtype for the time variable than currently supported: {}'.format(nc_fo))\n",
    "                \n",
    "print('...simulation DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Before we can get started, we have to prepare the sampled data such that it is compatible with the Machine Learning (ML) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a pandas dataframe from the dictionary and kick out rows with missing values as they do not work with ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = pd.DataFrame.from_dict(XY)\n",
    "print('number of data points including missing values:', len(XY))\n",
    "XY = XY.dropna()\n",
    "print('number of data points excluding missing values:', len(XY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert them to numpy arrays, separately for the variables (X) and the target conflict (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = XY.to_numpy()[:, :-1] # since conflict is the last column, we know that all previous columns must be variable values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = XY.conflict.astype(int).to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target evaluation\n",
    "\n",
    "Let's have a closer look at what we actualy work with. This is essential to select and tune the right ML model, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the total number of data points for our target is', len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_Y_1 = 100*len(np.where(Y != 0)[0])/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('from this, {0} points are equal to 1, i.e. represent conflict occurence. This is a fraction of {1} percent.'.format(len(np.where(Y != 0)[0]), round(fraction_Y_1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only a very small amout of conflicts are sampled. This small fraction compared to the vast amoutn of non-conflicts indicates we have an **imbalanced problem** and thus will need to account for this in the settings of the model used and data pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML modelling pipeline\n",
    "\n",
    "This pipeline allows to combine various scalers and models to find the best-performing combination and assess sensitivities.\n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "Before we can train and predict with the model, we need to scale the variable data. This is required because our input data has a range of units (or not) and values vary in orders of magnitude. \n",
    "\n",
    "Besides, it is important to create separate training- and testdata for both variables and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaler\n",
    "\n",
    "Depending on which scaler is chosen, the standardization of the data follows different approaches and may eventually influence model results. See here for some info: https://scikit-learn.org/stable/modules/preprocessing.html and https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html.\n",
    "\n",
    "The scaler is then used to fit the data and transform it according to scaler-specific method. I don't scale Y since it is either 0 or 1 already.\n",
    "\n",
    "The scaled variable data X_scaled is, together with the target data Y, split into trainings and test data. The fraction of the total data that is used for training is user-defined.\n",
    "\n",
    "The scatterplot of the first two variables in X_train looks like this. Also the sample size n_train is provided used to train the data alongside with the total variable sample size n_tot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = conflict_model.machine_learning.define_scaling(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "The variety of ML models is sheer endless. We here use two models for supervised learning which showed better performance than other candidates in previous assessment rounds. Also, all model parameters were already calibrated using GridSearchCV in previous analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = conflict_model.machine_learning.define_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pipeline\n",
    "\n",
    "The model pipeline starts with **splitting the data in scaled train and test samples**. \n",
    "\n",
    "It then contains a number of steps which are performed for each scaler-model combination, explained hereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***k-fold cross-validation***\n",
    "\n",
    "It is important to check how robust the models are in terms of accuracy as well as under- and overfitting. To that end, we apply **cross-validation (CV)** to fit the training-data against each other by splitting it up in chunks (as defined by k) and one chunk against all other k-1 chunks. k is 10 in our case, the default value is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Model fitting and prediction***\n",
    "\n",
    "Subsequently, we fit the models based on the training-data. With a fitted model, we can predict our target based on the remaining test-data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Evaluation***\n",
    "\n",
    "We have now produced a set of predictions, y_pred. This can be comapred with the retained test-targets y_test to evaluate the prediction. There are many ways to do this and several are applied here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** is either the fraction (default) or the count (normalize=False) of correct predictions.\n",
    "\n",
    "The **precision** is the ratio *tp / (tp + fp)* where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The **recall** is the ratio *tp / (tp + fn)* where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main classification metrics are nicely summarized in the **classification report**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision-Recall** is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n",
    "\n",
    "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n",
    "\n",
    "A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice way to vizualize the accuracy of our results is the **confusion matrix**. The confusion_matrix function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class. https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another metric is the **Brier score** (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss). The smaller the Brier score, the better, hence the naming with “loss”. Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the **F1 score**, also known as balanced F-score or F-measure. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1%20score#sklearn.metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open(os.path.join(out_dir, 'out.txt'), 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "for scaler in scalers:\n",
    "    \n",
    "    print('########################')\n",
    "    print('')\n",
    "    print(scaler)\n",
    "    print('')\n",
    "    \n",
    "    ##- scaling\n",
    "    X_s = scaler.fit_transform(X)\n",
    "\n",
    "    ##- splitting in train and test samples\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X_s,\n",
    "                                                                        Y,\n",
    "                                                                        test_size=1-config.getfloat('machine_learning', 'train_fraction'))\n",
    "    \n",
    "    for i, key in zip(range(X_train.shape[1]), config.items('env_vars')):\n",
    "\n",
    "        print('//////////////////////////////')\n",
    "        print('removing data for variable {}'.format(key[0]) + os.linesep)\n",
    "        X_train_loo = np.delete(X_train, i, axis=1)\n",
    "        X_test_loo = np.delete(X_test, i, axis=1)\n",
    "\n",
    "        sub_out_dir = os.path.join(out_dir, '_leave_one_out_analysis')\n",
    "        if not os.path.isdir(sub_out_dir):\n",
    "            os.makedirs(sub_out_dir)\n",
    "        sub_out_dir = os.path.join(sub_out_dir, '_excl_'+str(key[0]))\n",
    "        if not os.path.isdir(sub_out_dir):\n",
    "            os.makedirs(sub_out_dir)\n",
    "\n",
    "        plt.figure(figsize=(20,10))\n",
    "        sbs.scatterplot(x=X_train_loo[:,0],\n",
    "                        y=X_train_loo[:,1],  \n",
    "                        hue=y_train)\n",
    "\n",
    "        plt.title('training-data scaled with {0}; n_train={1}; n_tot={2}'.format(str(scaler).rsplit('(')[0], len(X_train_loo), len(X_s)))\n",
    "        plt.xlabel('Variable 1')\n",
    "        plt.ylabel('Variable 2')\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'scatter_plot_scaled_traindata_{}.png'.format(str(scaler).rsplit('(')[0])), dpi=300)\n",
    "\n",
    "        for clf in clfs:\n",
    "\n",
    "            print('====================================')\n",
    "            print('')\n",
    "            print(clf)\n",
    "            print('')\n",
    "\n",
    "            ##- k-fold cross-validation\n",
    "            accuracy = model_selection.cross_val_score(clf, X_train_loo, y_train, scoring='accuracy', cv=10)\n",
    "            print('the average k-fold cross-validation scores with k=10 for scaler {} and for this clf are {}'.format(scaler, accuracy.mean()))\n",
    "            print('')\n",
    "\n",
    "            ##- Fit the model with the scaled training data and the boolean conflict data\n",
    "            clf.fit(X_train_loo, y_train)\n",
    "\n",
    "            ##- Predict with the scaled prediction data\n",
    "            y_pred = clf.predict(X_test_loo)\n",
    "\n",
    "            ##- Evaluation\n",
    "            try:\n",
    "                y_score = clf.decision_function(X_test_loo)\n",
    "            except:\n",
    "                y_score = np.nan\n",
    "\n",
    "            print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "            print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "            print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "            print('')\n",
    "\n",
    "            print(metrics.classification_report(y_test, y_pred))\n",
    "            print('')\n",
    "\n",
    "            try:\n",
    "                average_precision = metrics.average_precision_score(y_test, y_score)\n",
    "                print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "            except:\n",
    "                print('Average precision-recall score could not be computed')\n",
    "                pass  \n",
    "\n",
    "            print('Brier score: to be implemented!')\n",
    "\n",
    "            try:\n",
    "                print('ROC score: {0:0.2f}'.format(metrics.roc_auc_score(y_test, y_score)))\n",
    "            except:\n",
    "                print('ROC score could not be computed')\n",
    "                pass\n",
    "\n",
    "            print('F1 score: {0:0.2f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "            print('')\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "            disp = metrics.plot_precision_recall_curve(clf, X_test_loo, y_test, ax=ax)\n",
    "            disp.ax_.set_title('2-class Precision-Recall curve with {} and {}; leaving out: {}'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0], key[0]))\n",
    "            plt.savefig(os.path.join(sub_out_dir, 'precision_recall_curve_{}+{}.png'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0])), dpi=300)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "            ax.set_title('confusion matrix with {} and {}; leaving out: {}'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0], key[0]))\n",
    "            metrics.plot_confusion_matrix(clf, X_test_loo, y_test, ax=ax)\n",
    "            plt.savefig(os.path.join(sub_out_dir, 'confusion_matrix_{}+{}.png'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0])), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "            ax.set_title('ROC curve with {} and {}; leaving out: {}'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0], key[0]))\n",
    "            metrics.plot_roc_curve(clf, X_test_loo, y_test, ax=ax)\n",
    "            plt.savefig(os.path.join(sub_out_dir, 'ROC_curve_{}+{}.png'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0])), dpi=300)               \n",
    " \n",
    "    print('//////////////////////////////')\n",
    "    print('ALL DATA')\n",
    "\n",
    "    sub_out_dir = os.path.join(out_dir, '_all_data')\n",
    "    if not os.path.isdir(sub_out_dir):\n",
    "        os.makedirs(sub_out_dir)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sbs.scatterplot(x=X_train[:,0],\n",
    "                    y=X_train[:,1],  \n",
    "                    hue=y_train)\n",
    "\n",
    "    plt.title('training-data scaled with {0}; n_train={1}; n_tot={2}'.format(str(scaler).rsplit('(')[0], len(X_train), len(X_s)))\n",
    "    plt.xlabel('Variable 1')\n",
    "    plt.ylabel('Variable 2')\n",
    "    plt.savefig(os.path.join(sub_out_dir, 'scatter_plot_scaled_traindata_{}.png'.format(str(scaler).rsplit('(')[0])), dpi=300)\n",
    "\n",
    "    for clf in clfs:\n",
    "\n",
    "        print('====================================')\n",
    "        print('')\n",
    "        print(clf)\n",
    "        print('')\n",
    "\n",
    "        ##- k-fold cross-validation\n",
    "        accuracy = model_selection.cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=10)\n",
    "        print('the average k-fold cross-validation scores with k=10 for scaler {} and for this clf are {}'.format(scaler, accuracy.mean()))\n",
    "        print('')\n",
    "\n",
    "        ##- Fit the model with the scaled training data and the boolean conflict data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        ##- Predict with the scaled prediction data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        ##- Evaluation\n",
    "        try:\n",
    "            y_score = clf.decision_function(X_test)\n",
    "        except:\n",
    "            y_score = np.nan\n",
    "\n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "        print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "        print('')\n",
    "\n",
    "        print(metrics.classification_report(y_test, y_pred))\n",
    "        print('')\n",
    "\n",
    "        try:\n",
    "            average_precision = metrics.average_precision_score(y_test, y_score)\n",
    "            print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "        except:\n",
    "            print('Average precision-recall score could not be computed')\n",
    "            pass  \n",
    "\n",
    "        print('Brier score: to be implemented!')\n",
    "\n",
    "        try:\n",
    "            print('ROC score: {0:0.2f}'.format(metrics.roc_auc_score(y_test, y_score)))\n",
    "        except:\n",
    "            print('ROC score could not be computed')\n",
    "            pass\n",
    "\n",
    "        print('F1 score: {0:0.2f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "        print('')\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "        disp = metrics.plot_precision_recall_curve(clf, X_test, y_test, ax=ax)\n",
    "        disp.ax_.set_title('2-class Precision-Recall curve with {} and {}'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0]))\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'precision_recall_curve_{}+{}.png'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0])), dpi=300)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "        ax.set_title('confusion matrix with {} and {}'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0]))\n",
    "        metrics.plot_confusion_matrix(clf, X_test, y_test, ax=ax)\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'confusion_matrix_{}+{}.png'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0])), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "        ax.set_title('ROC curve with {} and {}'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0]))\n",
    "        metrics.plot_roc_curve(clf, X_test, y_test, ax=ax)\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'ROC_curve_{}+{}.png'.format(str(scaler).rsplit('(')[0], str(clf).rsplit('(')[0])), dpi=300)\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All output, also print statements, are stored in the output folder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dubbelsteenmodel\n",
    "\n",
    "We have now assessed the performance of the scaler-model combination with actual data. The outcome is, however, limited by the available data and power of the scaler-model combination. \n",
    "\n",
    "Since it does very well in predicting non-conflict (0), it would be good to know what the upper limit is in this context. So, why not perform a small experiment?\n",
    "\n",
    "We set up a 'fake' yet comparable conflict datasets:\n",
    "\n",
    "* With the exact same fraction of conflicts as in actual dataset, but then randomly shuffled like by rolling a dice ('Y_r').\n",
    "\n",
    "We can then see how the main performance indicators compare to the run above using actual data. How good is applying a ML model compared to random prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the artificial array containing the exact same number of conflict as in the actual number, but then randomly shuffled.\n",
    "\n",
    "We first create an array with this amount of 'true' conflicts Nc1 and another array having the lenght of all conflicts-points N minus those with 'true' conflict Nc0. By appending both, we end up with an array having the exact same lengths as 'Y' containing the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1 = np.ones(len(np.where(Y != 0)[0]))\n",
    "arr_0 = np.zeros(int(len(Y) - len(np.where(Y != 0)[0])))\n",
    "Y_r_1 = np.append(arr_1, arr_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that the produced array really has the same amount of data points as the original Y array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(Y), len(Y_r_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for some heavy shuffling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_r = utils.shuffle(Y_r_1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that the number of 1s in the shuffled array is (still) identical to the number of 1s in the the original Y array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(np.where(Y_r != 0)[0]), len(np.where(Y != 0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can follow the same steps as in the run above, just with the fake Y instead of the actual Y.\n",
    "\n",
    "Note: this works only if sensitivity analysis is not activated as this would otherwise inflate the run time unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not config.getboolean('general', 'sensitivity_analysis'):\n",
    "    \n",
    "    sub_out_dir = os.path.join(out_dir, '_dubbelsteenmodel')\n",
    "    if not os.path.isdir(sub_out_dir):\n",
    "        os.makedirs(sub_out_dir)\n",
    "        \n",
    "    orig_stdout = sys.stdout\n",
    "    f = open(os.path.join(sub_out_dir, 'out.txt'), 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "\n",
    "    print('### DUBBELSTEENMODEL ###')\n",
    "    print('')\n",
    "    print(scalers)\n",
    "    print('')\n",
    "\n",
    "    ##- scaling\n",
    "    X_s = scalers[0].fit_transform(X)\n",
    "\n",
    "    ##- splitting in train and test samples\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X_s,\n",
    "                                                                        Y_r,\n",
    "                                                                        test_size=1-config.getfloat('machine_learning', 'train_fraction'))\n",
    "\n",
    "    print('====================================')\n",
    "    print('')\n",
    "    print(clfs[0])\n",
    "    print('')\n",
    "\n",
    "    ##- k-fold cross-validation\n",
    "    accuracy = model_selection.cross_val_score(clfs[0], X_train, y_train, scoring='accuracy', cv=10)\n",
    "    print('the average k-fold cross-validation scores with k=10 for scaler {} and for this clf are {}'.format(scalers[0], accuracy.mean()))\n",
    "    print('')\n",
    "\n",
    "    ##- Fit the model with the scaled training data and the boolean conflict data\n",
    "    clfs[0].fit(X_train, y_train)\n",
    "\n",
    "    ##- Predict with the scaled prediction data\n",
    "    y_pred = clfs[0].predict(X_test)\n",
    "\n",
    "    ##- Evaluation\n",
    "    try:\n",
    "        y_score = clfs[0].decision_function(X_test)\n",
    "    except:\n",
    "        y_score = np.nan\n",
    "\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print('F1 score: {0:0.2f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('')\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred) + os.linesep)\n",
    "\n",
    "    if i == 2:\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "        disp = metrics.plot_precision_recall_curve(clfs[0], X_test, y_test, ax=ax)\n",
    "        disp.ax_.set_title('2-class Precision-Recall curve with {} and {}; artificial Y'.format(str(scalers[0]).rsplit('(')[0], str(clfs[0]).rsplit('(')[0]))\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'precision_recall_curve_{}+{}.png'.format(str(scalers[0]).rsplit('(')[0], str(clfs[0]).rsplit('(')[0])), dpi=300)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "        ax.set_title('confusion matrix with {} and {}; artificial Y'.format(str(scalers[0]).rsplit('(')[0], str(clfs[0]).rsplit('(')[0]))\n",
    "        metrics.plot_confusion_matrix(clfs[0], X_test, y_test, ax=ax)\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'confusion_matrix_{}+{}.png'.format(str(scalers[0]).rsplit('(')[0], str(clfs[0]).rsplit('(')[0])), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "        ax.set_title('ROC curve with {} and {}; artificial Y'.format(str(scalers[0]).rsplit('(')[0], str(clfs[0]).rsplit('(')[0]))\n",
    "        metrics.plot_roc_curve(clfs[0], X_test, y_test, ax=ax)\n",
    "        plt.savefig(os.path.join(sub_out_dir, 'ROC_curve_{}+{}.png'.format(str(scalers[0]).rsplit('(')[0], str(clfs[0]).rsplit('(')[0])), dpi=300)\n",
    "            \n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
