{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook\n",
    "\n",
    "This notebook contains first bits and pieces of the yet to be developed model correlating climate/environmental factors with conflict occurrence.\n",
    "\n",
    "This notbook is under constant development. Please be aware of the version number of the conflict model used in each of the notebooks.\n",
    "\n",
    "In its current form, we first make a selection of conflicts to be used for training and testing the model. Selection criteria are amongst others minimum number of fatalities and climate zones. Subsequently, annual statistics (now: mean) of a range of environmental variables are determined per geographic unit (now: water provinces) and stored along with a 0/1 conflict value. This dataset is then scaled, split, and applied in a machine learning model (now: support vector classification).\n",
    "\n",
    "All model settings need to be defined in the run_settings.cfg file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and file with settings\n",
    "\n",
    "Import all required python packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conflict_model\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from configparser import RawConfigParser\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "import netCDF4 as nc\n",
    "import rasterstats as rstats\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import seaborn as sbs\n",
    "from sklearn import svm, preprocessing, model_selection, metrics\n",
    "from shutil import copyfile\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better reproducibility, the version numbers of all key packages are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.7 (default, Apr 15 2020, 05:09:04) [MSC v.1916 64 bit (AMD64)]\n",
      "conflict_model version: 0.0.2b2\n",
      "geopandas version: 0.8.0\n",
      "xarray version: 0.15.1\n",
      "rasterio version: 1.1.0\n",
      "pandas version: 1.0.3\n",
      "numpy version: 1.18.1\n",
      "scikit-learn version: 0.22.1\n",
      "matplotlib version: 3.2.1\n",
      "seaborn version: 0.10.1\n",
      "rasterstats version: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "conflict_model.utils.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geopandas versions lower than 0.7.0 do not yet have the clip function. The notebook will thus not work with these versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpd.__version__ < '0.7.0':\n",
    "    sys.exit('please upgrade geopandas to version 0.7.0, your current version is {}'.format(gpd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file all the settings for the analysis are defined. By 'parsing' it, all values are read for different sections. This is a simple way to make the code independent of the input data and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_file = r'../data/run_setting.cfg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RawConfigParser(allow_no_value=True)\n",
    "config.optionxform = lambda option: option\n",
    "config.read(settings_file);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the output folder as specified in the settings, in case it does not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the record, saving output to folder C:\\Users\\hoch0001\\Documents\\_code\\conflict_model\\data\\OUT\n"
     ]
    }
   ],
   "source": [
    "out_dir = config.get('general','output_dir')\n",
    "if not os.path.isdir(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "print('for the record, saving output to folder {}'.format(out_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the settings file in the output file to always get an idea on what settings the output is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyfile(settings_file, os.path.join(out_dir, 'copy_of_run_setting.cfg'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the conflict data base and convert it into a georeferenced dataframe. This is needed for all following steps where this data is combined with other data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv file to dataframe C:\\Users\\hoch0001\\Documents\\_code\\conflict_model\\data\\UCDP/ged191.csv\n",
      "...DONE\n",
      "\n",
      "translating to geopandas dataframe\n",
      "...DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf = conflict_model.utils.get_geodataframe(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, get the subset of conflicts based on user-defined conditions in the settings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering on conflict properties...\n",
      "...filtering key best with lower value 5\n",
      "...filtering key type_of_violence with value 1\n",
      "...passing key country as it is empty\n",
      "focussing on period between 2000 and 2015\n",
      "\n",
      "reading extent and spatial aggregation level from file C:\\Users\\hoch0001\\Documents\\_code\\conflict_model\\data\\waterProvinces/waterProvinces_Africa.shp\n",
      "...DONE\n",
      "\n",
      "clipping datasets to extent\n",
      "...DONE\n",
      "\n",
      "clipping to climate zones['BWh', 'BSh']\n",
      "...DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conflict_gdf, extent_gdf = conflict_model.selection.select(gdf, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Add functions to be tested here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis per year\n",
    "\n",
    "This is the core of the code. Here, we go through all model years as specified in the settings-file and do the following:\n",
    "\n",
    "1. Get a 0/1 classifier whether a conflict took place in a geographical unit or not;\n",
    "2. Loop through various files with environmental variables and get mean variable value per geographical unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all stored in a dictionary for easy processing. We first need to initialize this dictionary containing a pandas Series per provided environmental variable. Then, add a pandas Series for the conflict data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GDP_per_capita_PPP': Series([], dtype: float64),\n",
       " 'total_evaporation': Series([], dtype: float64),\n",
       " 'precipitation': Series([], dtype: float64),\n",
       " 'surface_water_storage': Series([], dtype: float64),\n",
       " 'upper_soil_storage': Series([], dtype: float64),\n",
       " 'groundwater_recharge': Series([], dtype: float64),\n",
       " 'temperature': Series([], dtype: float64),\n",
       " 'conflict': Series([], dtype: int32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = {}\n",
    "for key in config.items('env_vars'):\n",
    "    XY[str(key[0])] = pd.Series(dtype=float)\n",
    "XY['conflict'] = pd.Series(dtype=int)\n",
    "XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go through all years and all files and data and assign the values to the corresponding Series in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('simulation period from', str(config.getint('settings', 'y_start')), 'to', str(config.getint('settings', 'y_end')))\n",
    "print('')\n",
    "\n",
    "# go through all simulation years as specified in config-file\n",
    "for sim_year in np.arange(config.getint('settings', 'y_start'), config.getint('settings', 'y_end'), 1):\n",
    "    \n",
    "    print('entering year {}'.format(sim_year) + os.linesep)\n",
    "    \n",
    "    # go through all keys in dictionary\n",
    "    for key, value in XY.items():\n",
    "        \n",
    "        if key == 'conflict':\n",
    "            pass #for now\n",
    "            data_series = value\n",
    "            data_list = conflict_model.get_boolean_conflict.conflict_in_year_bool(conflict_gdf, extent_gdf, config, sim_year)\n",
    "            data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "            XY[key] = data_series\n",
    "            \n",
    "        else:\n",
    "            nc_fo = os.path.join(config.get('general', 'input_dir'), \n",
    "                                 config.get('env_vars', key))\n",
    "            \n",
    "            print('calculating mean {0} per aggregation unit from file {1} for year {2}'.format(key, nc_fo, sim_year))\n",
    "\n",
    "            nc_ds = xr.open_dataset(nc_fo)\n",
    "            \n",
    "            print(nc_ds.time.dtype)\n",
    "            \n",
    "            if not isinstance(nc_ds.time.dtype, np.datetime64):\n",
    "                print('boring')\n",
    "#                 data_series = value\n",
    "#                 data_list = conflict_model.get_var_from_nc.nc_with_integer_timestamp(extent_gdf, config, key, sim_year)\n",
    "#                 data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "#                 XY[key] = data_series\n",
    "                \n",
    "            elif:\n",
    "                print('GDPPPP')\n",
    "#                 data_series = value\n",
    "#                 data_list = conflict_model.get_var_from_nc.nc_with_continous_regular_timestamp(extent_gdf, config, key, sim_year)\n",
    "#                 data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "#                 XY[key] = data_series\n",
    "                \n",
    "print('...simulation DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('simulation period from', str(config.getint('settings', 'y_start')), 'to', str(config.getint('settings', 'y_end')))\n",
    "# print('')\n",
    "\n",
    "# # go through all simulation years as specified in config-file\n",
    "# for sim_year in np.arange(config.getint('settings', 'y_start'), config.getint('settings', 'y_end'), 1):\n",
    "    \n",
    "#     print('entering year {}'.format(sim_year) + os.linesep)\n",
    "    \n",
    "#     # go through all keys in dictionary\n",
    "#     for key, value in XY.items():\n",
    "        \n",
    "#         if key == 'conflict':\n",
    "#             data_series = value\n",
    "#             data_list = conflict_model.get_boolean_conflict.conflict_in_year_bool(conflict_gdf, extent_gdf, config, sim_year)\n",
    "#             data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "#             XY[key] = data_series\n",
    "#         elif key == 'GDP_per_capita_PPP':\n",
    "#             data_series = value\n",
    "#             data_list = conflict_model.get_var_from_nc.nc_with_integer_timestamp(extent_gdf, config, key, sim_year)\n",
    "#             data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "#             XY[key] = data_series\n",
    "#         else:\n",
    "#             data_series = value\n",
    "#             data_list = conflict_model.get_var_from_nc.nc_with_continous_regular_timestamp(extent_gdf, config, key, sim_year)\n",
    "#             data_series = data_series.append(pd.Series(data_list), ignore_index=True)\n",
    "#             XY[key] = data_series\n",
    "    \n",
    "# print('...simulation DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a pandas dataframe from the dictionary and kick out rows with missing values (they do not work with ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = pd.DataFrame.from_dict(XY)\n",
    "print('number of data points including missing values:', len(XY))\n",
    "XY = XY.dropna()\n",
    "print('number of data points excluding missing values:', len(XY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert them to numpy arrays, separately for the variables (X) and the target conflict (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = XY.to_numpy()[:, :-1] # since conflict is the last column, we know that all previous columns must be variable values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = XY.conflict.astype(int).to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target evaluation\n",
    "\n",
    "Let's have a closer look at what we actualy work with. This is essential to select and tune the right ML model, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the total number of data points for our target is', len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('from this, {0} points are equal to 1, i.e. represent conflict occurence. This is a fraction of {1} percent.'.format(len(np.where(Y != 0)[0]), round(100*len(np.where(Y != 0)[0])/len(Y), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Before we can train and predict with the model, we need to scale the variable data and create trainings and test data for both variables and target.\n",
    "\n",
    "There are different scaling algorithms available. For our application, the MinMaxScaler and StandardScaler are the most obvious choices, the RobustScaler is follow-up. Depending on how incoming data looks like and is distributed, the scaling decision may need to be updated.\n",
    "\n",
    "Depending on which scaler is chosen, the standardization of the data follows different approaches and may eventually influence model results. See here for some info: https://scikit-learn.org/stable/modules/preprocessing.html and https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# scaler = preprocessing.RobustScaler()\n",
    "scaler = preprocessing.QuantileTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler is then used to fit the data and transform it according to scaler-specific method. I don't scale Y since it is either 0 or 1 already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled variable data X_scaled is, together with the target data Y, split into trainings and test data. The fraction of the total data that is used for training is user-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_scaled,\n",
    "                                                                    Y,\n",
    "                                                                    test_size=1-config.getfloat('machine_learning', 'train_fraction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot of the first two variables in X_train looks like this. Also the sample size n_train is provided used to train the data alongside with the total variable sample size n_tot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sbs.scatterplot(x=X_train[:,0],\n",
    "                y=X_train[:,1],  \n",
    "                hue=y_train)\n",
    "\n",
    "plt.title('training-data scaled with {0}; n_train={1}; n_tot={2}'.format(str(scaler).rsplit('(')[0], len(X_train), len(X_scaled)))\n",
    "plt.xlabel('Variable 1')\n",
    "plt.ylabel('Variable 2')\n",
    "plt.savefig(os.path.join(out_dir, 'scatter_plot_scaled_traindata_{}.png'.format(str(scaler).rsplit('(')[0])), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief look at the statistics of the data of our scaled and standardized variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.mean(axis=0), X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Train and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Support Vector Classification (SVC) model with balanced weight since data is unbalanced (e.g. many negative and few positive).\n",
    "\n",
    "Note that there are many many settings in the svm.SVC class to be altered. At the moment, we stick to defaults besides the class_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with the scaled training data and the boolean conflict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with the scaled prediction data. We see an array with lots of 0 and 1 is produced, similar as the target data of the model, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clue right now what this does... look it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = clf.decision_function(X_test)\n",
    "y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** is either the fraction (default) or the count (normalize=False) of correct predictions.\n",
    "\n",
    "The **precision** is the ratio *tp / (tp + fp)* where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The **recall** is the ratio *tp / (tp + fn)* where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision-Recall** is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n",
    "\n",
    "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n",
    "\n",
    "A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = metrics.average_precision_score(y_test, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "disp = metrics.plot_precision_recall_curve(clf, X_test, y_test, ax=ax)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: AP={} with {}'.format(round(average_precision,2), str(scaler).rsplit('(')[0]))\n",
    "plt.savefig(os.path.join(out_dir, 'precision_recall_curve_{}.png'.format(str(scaler).rsplit('(')[0])), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are pretty crappy, but that is okay give we use not very sensible input data at the moment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "Let's safe some settings used in this run to csv-files so results can be assessed in light of these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the parameters of our SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_params = clf.get_params()\n",
    "\n",
    "out_fo = os.path.join(out_dir, 'SVC_params.csv')\n",
    "w = csv.writer(open(out_fo, \"w\"))\n",
    "for key, val in SVC_params.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the parameters of the scaling method we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_params = scaler.get_params()\n",
    "\n",
    "out_fo = os.path.join(out_dir, '{}_params.csv'.format(str(scaler).rsplit('(')[0]))\n",
    "w = csv.writer(open(out_fo, \"w\"))\n",
    "for key, val in scaler_params.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, the resulting values of our objective functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {'Accuracy': round(metrics.accuracy_score(y_test, y_pred), 2),\n",
    "              'Precision': round(metrics.precision_score(y_test, y_pred), 2),\n",
    "              'Recall': round(metrics.recall_score(y_test, y_pred), 2),\n",
    "              'Average precision-recall score': round(average_precision, 2)}\n",
    "\n",
    "out_fo = os.path.join(out_dir, 'evaluation.csv')\n",
    "w = csv.writer(open(out_fo, \"w\"))\n",
    "for key, val in evaluation.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
